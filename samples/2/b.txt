Москва
2012
Московский Государственный Университет
имени М.В. Ломоносова
Факультет вычислительной математики и кибернетики
Кафедра системного программирования
Курсовая работа
Исследование и разработка методов поиска плагиата в многоязычных
корпусах текстов
Выполнила:
Астапова Оксана Петровна
Группа 427
Научный руководитель:
к.ф. – м.н. Турдаков Денис Юрьевич2
Оглавление
Аннотация ......................................................................................................................................3
Введение.........................................................................................................................................4
1 Постановка задачи......................................................................................................................6
2 Обзор существующих решений ................................................................................................7
2.1 Метод «шинглов» ................................................................................................................7
2.2 Использование тезауруса ....................................................................................................8
2.3 I-Match...................................................................................................................................8
2.4 Слова с оптимальным весом...............................................................................................9
2.5 Наиболее часто встречающиеся слова.............................................................................10
2.6 Эффективность рассмотренных методов ........................................................................10
3 Исследование и построение решения.....................................................................................12
4 Описание практической части.................................................................................................14
4.1 Обоснование выбранного инструментария.....................................................................14
4.2 Общая схема работы..........................................................................................................14
4.3 Архитектура системы........................................................................................................16
4.4 Характеристики функционирования................................................................................17
4.4.1 Эффективность................................................................................................................17
4.4.2 Производительность .......................................................................................................18
Заключение...................................................................................................................................19
Список литературы......................................................................................................................203
Аннотация
Целью данной работы было исследование алгоритмов поиска плагиата в многоязычных
корпусах текстов и реализация подобного алгоритма для корпуса, в котором содержатся
документы на русском и английском языках. Были также рассмотрены существующие
решения для текстов на одном языке и возможные пути их использования для
многоязычных корпусов. Некоторые из рассмотренных алгоритмов были реализованы с
небольшими изменениями в прототипе системы поиска плагиата в многоязычных
корпусах.4
Введение
В современном информационном обществе проблема определения оригинальности текста
занимает значительное место. Как правило, в данном случае подразумевается
установление нарушения авторских прав, однако это не единственная проблемная
область: возможность определения сходства текстовых документов позволяет улучшать
качество работы поисковых систем за счет удаления избыточной информации,
фильтровать поисковый и почтовый спам, кластеризовать тексты по содержанию.
Большие объемы обрабатываемых данных делают задачу поиска похожих текстов
алгоритмически сложной. При этом алгоритмы, успешно работающие для конкретной
постановки задачи, бывают неприменимы или дают плохие результаты для других задач.
Так, например, в юрислингвистике для определения авторства текста изучают его
стилистические особенности, категоричность высказываний, использование оценочной
лексики. Очевидно, данный подход будет бесполезен при работе с текстами на различных
языках. Поэтому важно четко определить понятия плагиата и похожих текстов.
Wikipedia дает следующее определение: — умыш енное пр своен е в орс в
чужо о про зведен я н ук скусс в , чуж х дей зобре ен й. може
бы ь н рушен ем в орско-пр вово о з конод е ьс в п ен но о
з конод е ьс в в к чес ве ковых може пов ечь з собой юр д ческую
о ве с веннос ь. С дру ой с ороны, п возможен в об с ях, н ко орые не
р спрос р няе ся дейс в е к к х- бо в дов н е ек у ьной собс веннос ,
н пр мер, в м ем ке дру х фунд мен ьных н учных д сц п н х.
выр ж е ся в пуб к ц под сво м менем чужо о про зведен я чуж х дей,
 кже в з мс вов н фр мен ов чуж х про зведен й без ук з н я с очн к
з мс вов н я».
Литературный словарь: — пр своен е п одов чужо о ворчес в :
опуб ков н е чуж х про зведен й под сво м менем без ук з н я с очн к
 спо ьзов н е без преобр зующ х ворческ х зменен й, внесенных з мс вов е ем»
Словарь иностранных слов русского языка: ЛАГИАТ — ( .). Воровс во
 ер урной собс веннос : мыс ей, р д»
Викисловарь (для слова plagiarism): Коп ров н е дей, екс дру ой ворческой
р бо ы дру о о че овек , предс в ен е х к к сво х собс венных, особенно без е о
р зрешен я»
В данной работе будем придерживаться первого определения, поскольку оно в той или
иной форме приводится в большинстве статей, описывающих системы и алгоритмы
определения плагиата. Также при описании алгоритмов поиска похожих документов,
наравне с данным термином, часто используется термин «нечеткие дубликаты».
Таким образом, для разработки алгоритма важно определить два аспекта:
1) Критерий определения похожести текстов (форма или содержание)
2) Определение оценки степени похожести и ее порогового значения, до которого
тексты не считаются дубликатами.5
Для автоматической обработки многоязыковых корпусов текстов целесообразнее
рассматривать заимствование содержания, т.к. сам процесс перевода уже значительно
изменяет форму изложения. Что касается второго пункта, возможность использования
ссылок и цитации усложняет принятие однозначного решения, поэтому будем говорить о
системах поиска плагиата, которые обращают внимание эксперта на возможные случаи
заимствования, предоставляя ему окончательное решение.
Введем определения плагиата и дубликатов, которые будут использоваться далее в данной
работе.
Нечеткие дубликаты – тексты, содержание которых отличается незначительно или не
отличается вовсе. Будем также говорить о дубликатах, если информация, содержащаяся в
одном тексте, полностью содержится в другом.
Плагиат – явление заимствования содержимого одного текста в другом.
Согласно классификации, приведенной в [5], случаи плагиата можно разделить на две
основные группы: точное копирование и копирование с модификацией. Во втором случае
текст может быть переформулирован или переведен на другой язык. Кроме того, может
быть скопирован как целый текст, так и его часть. Алгоритмы, успешно обнаруживающие
плагиат одного типа, могут давать очень слабые результаты для плагиата других типов. В
данной работе рассматривается переводной плагиат (translational plagiarism) – явление
копирования целого текста, в котором основной модификацией является перевод на
другой язык. При исследовании данной проблемы нам также понадобится определить
понятие тезауруса.
Тезаурус – словарь, сопоставляющий различным словам, описывающим одно и то же
понятие на разных языках, один дескриптор.
Заметим, что используется упрощенное понятие тезауруса, т.к. не учитывается
взаимосвязь с другими понятиями и охват всех слов языка не является целью его
построения.
Поскольку определение текста-источника и текста-заимствования является отдельной
задачей, не всегда разрешимой, будем говорить о парах дубликатов (схожих текстах).
Итак, в данной работе речь будет идти о методах поиска пар дубликатов текстов в
многоязычных корпусах текстов.6
1 Постановка задачи
Целью данной работы является исследование и разработка методов поиска плагиата в
многоязычных корпусах текстов. Для достижения данной цели были поставлены
следующие задачи:
1. Исследовать существующие методы поиска плагиата в многоязычных корпусах
текстов.
2. Разработать алгоритм поиска плагиата в корпусах, содержащих тексты на
английском и русском языках.
3. Создать прототип системы поиска плагиата, подтверждающий работоспособность
данного метода.7
2 Обзор существующих решений
Существует ряд методов для нахождения плагиата в многоязычных корпусах текстов.
Наиболее часто процесс поиска осуществляется по следующему алгоритму:
1. Эвристический поиск. Из корпуса отбираются документы, в которых содержатся
фрагменты текста, похожие на некоторые части текста Т0, проверяемого на
плагиат. На этом шаге используются различные алгоритмы для обнаружения
похожих текстов, в том числе часто необходимо использовать некоторый переход
между языками документов – перевод ключевых слов, определение жанра и темы и
др.
2. Подробный анализ. Для каждого из отобранных документов анализируется
степень сходства с проверяемым, если она достаточно высока – предполагается
случай плагиата. Во многих системах этот шаг применяется без эвристического
поиска: сравнение идет с каждым из текстов корпуса.
3. Пост-обработка. Полученные результаты проверяются, чаще всего вручную, для
исключения ложных обнаружений (например, случаев, когда за плагиат
принимается оформленная по всем правилам цитация).
Рассмотрим наиболее часто используемые приемы отбора текстов из корпуса:
1. В тексте Т0 выделяются ключевые слова, которые затем переводятся на языки
каждого из рассматриваемых документов корпуса, производится поиск текстов по
этим ключевым словам.
2. Текст Т0 переводится на язык документа, с которым сравнивается, при помощи
машинного перевода, после чего из него выделяются ключевые слова, происходит
поиск полученных слов в документе. Эти два подхода сравнимы по эффективности.
3. Текст представляется как небольшой набор чисел, называемый отпечатком. Эти
числа вычисляются при помощи hash-функции, реализующей функцию меры
близости и с высокой вероятностью сопоставляющей похожие документы
одинаковому hash-коду. При использовании данного метода hash-код строится, как
правило, по машинному переводу текста Т0 на язык документа, с которым идет
сравнение, однако некоторые исследования предлагают хеш-функции, при помощи
которых можно получить одинаковые результаты для похожих текстов на разных
языках.
Довольно часто для поиска плагиата в многоязыковых корпусах используется сочетание
машинного перевода и алгоритмов для текстов на одном языке. Рассмотрим подробнее
наиболее популярные из них, а также некоторые алгоритмы, которые описываются в [1]
как наиболее эффективные.
2.1 Метод «шинглов»
Одним из наиболее часто используемых является метод «шинглов», предложенный в 1997
году. Он основан на представлении документа в виде всевозможных последовательностей
фиксированной длины k, состоящих из соседних слов. Такие последовательности назвали
«шинглами» (от англ. shingles). Два документа считаются похожими, если множества их 8
шинглов существенно пересекаются. Поскольку число шинглов для каждого документа
является достаточно большим, используются различные способы усечения их множества,
основанные на вычислении их дактилограмм. Дактилограмма документа включает все
текстовые подстроки фиксированной длины, а ее численное значение при помощи
алгоритма случайных полиномов Карпа-Рабина.
В современной интерпретации каждый документ представляется 84 шинглами,
минимизирующими статистические функции, использованные для вычисления
дактилограмм. Эти шинглы разбиваются на 6 групп по 14 шинглов в каждой –
супершинглы. Если два документа имеют сходство с вероятностью р, то два их
супершингла совпадают с вероятностью p14. Для эффективной проверки гипотезы о
схожести каждый документ представляется всевозможными попарными сочетаниями из 6
супершинглов – мегашинглами (их всего 15). Два документа считаются схожими по
содержанию, если у них совпадает хотя бы один мегашингл.
Методы на основе шинглов используются в основе многих алгоритмов определения
плагиата в одноязычных корпусах, однако для случая многих языков их применение
нецелесообразно, т.к. изменение порядка слов при переводе не позволяет
абстрагироваться от языка, на котором написан текст.
2.2 Использование тезауруса
Еще одно из интересных решений предложено Bruno Pouliquen в [4]. На первом этапе,
каждому дескриптору из тезауруса EUROVOC (http://eurovoc.europa.eu/) ставится в
соответствие статистически определяемый набор слов, относящихся к нему. Далее для
каждого из рассматриваемых текстов определяется некоторый набор соответствующих
ему дескрипторов из EUROVOC и эти наборы сравниваются. Похожим текстам будут
соответствовать похожие наборы дескрипторов. Данный метод интересен тем, что не
требует машинного перевода ни для проверяемого документа, ни для документов корпуса,
кроме того, наборы дескрипторов для документов корпуса достаточно вычислить один
раз. Однако использование такого алгоритма ограничивается набором языков, для
которых доступен тезаурус.
2.3 I-Match
Рассмотрим другой сигнатурный подход, основанный уже не на синтаксических, а на
лексических принципах. Основная идея такого подхода состоит в вычислении
дактилограммы I-Match для представления содержания документов. С этой целью сначала
для исходной коллекции документов строится словарь L, который включает слова со
средними значениями IDF, поскольку такие слова обеспечивают, как правило, более
точные результаты при обнаружении нечетких дубликатов. Слова с большими и
маленькими значениями IDF отбрасываются. Затем для каждого документа формируется
множество U различных слов, входящих в него, и определяется пересечение U и словаря
L. Если размер этого пересечения больше некоторого минимального порога 9
(определяемого экспериментально), то список слов, входящих в пересечение
упорядочивается, и для него вычисляется I-Match сигнатура (hash-функция SHA1).
Два документа считаются похожими, если у них совпадают I-Match сигнатуры (имеет
место коллизия hash-кодов). Алгоритм имеет высокую вычислительную эффективность,
превосходящую показатели алгоритма шинглов. Другим преимуществом алгоритма
(также, по сравнению с алгоритмом шинглов, предложенным A. Broder) является его
высокая эффективность при сравнении небольших по размеру документов. Основной
недостаток — неустойчивость к небольшим изменениям содержания документа. Для
преодоления указанного недостатка исходный алгоритм был модифицирован авторами
статьи [1], и в него была введена возможность многократного случайного перемешивания
основного словаря. Дополнительно к основному словарю L создаются K различных
словарей L1-LK, получаемых путем случайного удаления из исходного словаря
некоторого небольшой фиксированной части p слов, составляющей порядка 30%-35% от
исходного объема L.
Для каждого документа вместо одной вычисляется (K+1) I-Match сигнатура по алгоритму,
описанному выше, т.е. документ представляется в виде вектора размерности (K+1), и два
документа считаются дубликатами, если у них совпадает хотя бы одна из координат.
Если документ подвергается небольшим изменениям (порядка n слов), то вероятность
того, что по крайней мере одна из K дополнительных сигнатур останется неизменной,
будет равна:
1 - (1-p
n
)
K (1)
Действительно, вероятность того, что изменения не затронут какого-либо одного словаря,
равна pn — вероятности, что все изменения попадут в удаленную часть исходного
словаря. Тогда (1-p
n
) — вероятность, что сигнатура изменится, а (1-p
n
)
K — вероятность,
что все сигнатуры изменятся (поскольку дополнительные словари формируются
независимо), и, следовательно, (1) — и есть искомая вероятность.
Рекомендуемыми значениями параметров, хорошо проявившими себя на практике,
являются p = 0.33 и K = 10.
Согласно статье [1], алгоритм показал высокие результаты при использовании в
различных приложениях веб-поиска и фильтрации спама.
2.4 Слова с оптимальным весом
Алгоритм реализует метод «оптимальной поисковой частоты», предложенный М.
Масловым, и использующийся для поиска похожих документов в широком спектре
приложений, от веб-поисковика до кластеризации новостей. Суть его заключается в
следующем. Вместо классической метрики TF*IDF предлагается ее модифицированный
вариант. Вводится эвристическое понятие «оптимальной частоты» для слова равное –
ln(10/1000000) = 11.5, т.е. «оптимальным» считается вхождение слова в 10 документов из 10
1000000. Если реальное значение IDF меньше «оптимального», то оно немного (по закону
параболы) повышается до
IDF_opt = SQRT(IDF / 11.5), (2)
а если больше, то существенно (как гипербола) снижается до
IDF_opt= 11.5 / IDF. (3)
Таким образом по всей коллекции строится словарь, ставящий каждому слову в
соответствие число документов, в которых оно встречается хотя бы один раз (df).
Далее строится частотный словарь документа и для каждого слова вычисляется его «вес»
wt по формуле:
wt = TF * IDF_opt, где TF = 0.5 + 0.5 * tf / tf_max (4)
IDF = -log(df / N) (5)
IDF_opt = SQRT(IDF / 11.5), (6)
если IDF меньше чем 11.5, иначе
IDF_opt = 11.5 / IDF. (7)
Затем выбираются и сцепляются в алфавитном порядке в строку 6 слов с наибольшими
значениями wt.
В качестве сигнатуры документа вычисляется контрольная сумма CRC32 полученной
строки.
2.5 Наиболее часто встречающиеся слова
В качестве сигнатуры документа используется хэш-код строки, полученной сцеплением
шести наиболее часто встречающихся в документа слов. Данный метод можно считать
одним из случаев использования анализа ключевых слов текстов. Несмотря на свою
простоту, при использовании нормализации и стоп-слов метод дает неплохие результаты.
2.6 Эффективность рассмотренных методов
В таблице 1 приводится сравнение эффективности методов 3-5 при обработке коллекции
веб-сайтов РОМИП (~500 000 документов), данные представлены в соответствии с [1].
Таблица 1 – Эффективность рассмотренных методов
Алгоритм Полнота Точность F-мера
iMatch (K=10) 0.50 0.97 0.66
Оптимальный вес 0.59 0.94 0.73
Максимальная частота 0.60 0.94 0.7311
Данные алгоритмы были выбраны для рассмотрения среди 12 алгоритмов, описываемых в
работе И. Сегаловича, поскольку имеют наибольшую F-меру (а iMatch – максимальную
точность) из тех методов, которые могут быть использованы для поиска плагиата в
многоязычном корпусе.
Метод основанный на использовании тезауруса EUROVOC, согласно [4], показал
точность от 0.62 до 0.97 для разных комбинаций языков.
Из рассмотренных методов более всего для решения поставленных задач подходят методы
iMatch, метод максимальной частоты и метод оптимального веса. Это связано с тем, что
данные алгоритмы не учитывают семантических особенностей текста, которые
значительно изменяются при переводе. Однако результат применения указанных методов
может иметь низкую полноту, поскольку индикатором дубликатов является hash-
коллизия, а перевод текста может несколько изменить статистические отношения между
словами в нем. Далее будут описаны модификации методов, которые позволят достичь
поставленную цель.12
3 Исследование и построение решения
Проблему поиска плагиата в многоязычном корпусе можно разбить на две подзадачи:
приведение текстов к единому языку (или набору дескрипторов) и определение степени
похожести между ними.
В качестве первого шага была использована упрощенная и несколько модифицированная
идея 2.2 (алгоритма, основанного на тезаурусе): каждому слову вместе с его вариантами
на всех языках корпуса сопоставляется дескриптор, и тезаурус строится в процессе
обработки корпуса. Каждое новое слово заносится в префиксное дерево со следующим по
порядку дескриптором, либо с уже имеющимся, если в дереве присутствует его перевод.
Таким образом каждый текст можно представить в виде вектора целых чисел. Такое
представление позволяет абстрагироваться от языка, на котором написан текст, однако
налагает некоторые ограничения на используемые на следующем шаге алгоритмы
обнаружения плагиата: некоторые эффективные для одного языка алгоритмы невозможно
использовать, т.к. в них рассматриваются такие критерии как длины слов и предложений.
Для перевода был использован Google Translate, т. к. он довольно успешно справляется с
ненормализованным текстом.
Как уже было сказано выше, многие методы обнаружения плагиата в одноязычных
корпусах могут быть использованы для случая многих языков с некоторыми
изменениями. Для реализации в данной работе были выбраны iMatch с расширением на 5
словарей, наиболее частые слова (далее FreqWords) и слова с оптимальным весом (далее
OptWeight). К этим методам был добавлен косинус между векторами слов: при простоте
реализации легко эмпирически определить порог, выше которого считать тексты
возможным плагиатом.
Однако при реализации этих методов так, как описано в [1] оказалось, что из-за
особенностей машинного перевода их полнота очень низкая: все три (кроме косинуса)
метода определяли возможным плагиатом только точную копию текста на том же языке.
Поэтому было решено внести в реализацию алгоритмов следующие изменения.
Во всех трех алгоритмах было решено отказаться от использования hash-функции для
определения степени похожести в пользу косинуса для iMatch и подсчета количества
совпадающих элементов множества для двух других методов. Такая замена позволила
более гибко устанавливать порог, при превышении которого тексты считаются
дубликатами.
Для алгоритма iMatch других изменений не потребовалось.
В алгоритме FreqWords выборка из 10 слов с максимальной частотой рассматривается как
множество. Если совпадает не менее 6 из них – тексты считаются возможными
дубликатами. Такой выбор рассматриваемого количества обусловлен эмпирическими
результатами, которые можно наблюдать в таблице 2.13
Таблица 2 – Зависимость эффективности алгоритма FreqWords от параметров
Выборка, слов Количество
совпадений
Точность Полнота F-мера
9 5 0.86 0.61 0.71
9 6 0.98 0.54 0.69
10 5 0.75 0.7 0.72
10 6 1.0 0.61 0.76
10 7 1.0 0.53 0.69
11 5 0.59 0.77 0.67
11 6 0.85 0.67 0.75
11 7 1.0 0.53 0.69
В алгоритме optWeight выборка из 8 слов с оптимальным весом рассматривается как
множество. Если совпадает не менее 4 из них – тексты считаются возможными
дубликатами. Результат работы данного алгоритма для других параметров можно
наблюдать в таблице 3.
Таблица 3 – Зависимость эффективности алгоритма optWeight от параметров
Выборка, слов Количество
совпадений
Точность Полнота F-мера
7 3 0.4 0.8 0.53
7 4 0.89 0.65 0.75
8 4 0.82 0.75 0.785
8 5 0.97 0.66 0.787
9 4 0.65 0.78 0.71
9 5 0.93 0.68 0.786
Наилучшим результатом выбрано совпадение 4 слов из 8, поскольку оно имеет
наибольшую полноту среди наборов параметров с максимальной и близкой к ней F-мере.
Таким образом, реализация методов для моноязыковых корпусов с некоторыми
модификациями позволила решить поставленную задачу. Далее будут рассмотрены
подробности реализации и общая схема работы построенного прототипа системы.14
4 Описание практической части
4.1 Обоснование выбранного инструментария
В качестве языка реализации был выбран Java, являющийся кроссплатформенным языком,
подходящим для разработки встраиваемого в систему модуля для обработки текста. К
тому же решаемая задача не требует непосредственной работы с памятью.
Для перевода используется API для работы с Google Spreadsheets
(https://developers.google.com/google-apps/spreadsheets/?hl=ru). Возможность использования
формулы GoogleTranslate(“<слово>”, “<язык оригинала>”, “<язык результата>”)
позволила проводить неограниченное количество тестов при разработке, в отличие от
других доступных API для машинного перевода. Кроме того, Google Translate успешно
обрабатывает ненормализованный текст, поэтому шаг нормализации был опущен при
реализации прототипа системы.
Для хранения связей между словами на разных языках и дескрипторами использован
класс Trie (https://sites.google.com/site/indy256/algo/trie), для которого единственной
модификацией стало добавление возможности сериализации.
4.2 Общая схема работы
1. Обработка корпуса и построение тезауруса.
1.1 Для каждого из текстов корпуса производятся следующие действия:
1.1.1 Для каждого слова, имеющего длину более 2 букв, определяется его
дескриптор: если оно, либо его перевод уже есть в дереве дескрипторов, то дескриптор
берется из дерева. Если слово (и его перевод) еще не встречалось в текстах корпуса – оно
заносится в дерево, получая следующий по порядку дескриптор.
1.1.2 В результате замены слов на дескрипторы каждому тексту сопоставляется
вектор целых чисел, где индекс элемента соответствует дескриптору слова, а значение
элемента – числу вхождений слова в данный текст.
1.2 Для всего корпуса:
1.2.1 В процессе обработки вычисляются два дополнительных вектора: df и tf_max.
df[i] – общее количество вхождений всех слов с дескриптором i в тексты корпуса.
tf_max[i] – максимальное количество вхождений слова с дескриптором i в один
текст.
1.2.2 Полученное дерево дескрипторов и векторы, представляющие тексты, а также
векторы df и tf_max сохраняются в файлы trie.out, tfs.txt, df_tf_max.txt для возможности
дальнейшего использования без повторной обработки корпуса.
1.3 Если корпус уже был обработан ранее, то из файлов trie.out, tfs.txt, df_tf_max.txt
загружается дерево дескрипторов и векторы, представляющие все файлы корпуса, а также
векторы общего и максимального количества вхождений.15
2. Обработка текста-подозреваемого. Происходит так же, как для всех текстов корпуса, на
выходе получается вектор дескрипторов для данного текста, а также обновленные вектора
df и tf_max.
3. В зависимости от параметров запуска выполняется один или несколько алгоритмов
поиска плагиата.
3.1 Косинус. Для каждой пары «подозреваемый + текст из корпуса» рассчитывается
значение косинуса между их векторами дескрипторов. Если оно превышает 0.63, тексты
считаются дубликатами.
3.2 iMatch
3.2.1 Для всех слов корпуса считается idf по формуле из обзора
существующих решений. 1\6 часть слов с наименьшим и 1\6 часть слов с
наибольшим значением отбрасываются. Получается основной словарь.
3.2.2 По основному словарю строятся 5 дополнительных, в которых
случайным образом отбрасываются еще до 30% слов.
3.2.3 Для каждого текста корпуса и текста-подозреваемого находятся и
сравниваются пересечения с шестью построенными словарями. Если для некоторой
пары текстов косинус векторов пересечений хотя бы с одним словарем превышает
0.6 – тексты считаются дубликатами.
3.3 freqWords. Для каждого текста выбираются 10 наиболее часто встречающихся в
нем слов. Если для пары текстов среди этих десяти находится хотя бы 6 совпадений –
тексты считаются дубликатами.
3.4 optWeight. Для каждого текста выбираются 8 слов с наибольшим оптимальным
весом, согласно формуле из п. 2.4. Пара текстов считается дубликатами, если среди этих
шести совпадают хотя бы 4 слова.
Помимо запуска для одного текста-подозреваемого можно анализировать весь корпус
целиком. В этом случае шаг 2 пропускается и в роли текста-подозреваемого по очереди
выступает каждый из текстов корпуса.
Краткая схема работы системы приведена на рисунке 1.16
Рисунок 1 - Схема работы системы
4.3 Архитектура системы
Схема архитектуры системы представлена на рисунке 2.
Класс Antiplagiarist является основным классом и оболочкой для запуска различных
этапов поиска плагиата.
Класс Trie реализует структуру префиксного дерева, с возможностью вставки и удаления
элементов, получения значений, хранящихся в узлах.
Функции getTranslate() и putTranslate() реализуют API для машинного перевода.
Функция config_rebuild(config, path) реализует обработку корпуса: построение и
сохранение дерева дескрипторов, расчет статистики по корпусу. Config – файл,
содержащему список имен файлов с текстами корпуса вместе с их языками, path – путь к
текстам корпуса.
Функция config_restore(path, vocTrie, fnames, tf, df, tf_max) загружает построенное ранее
дерево дескрипторов и статистику по корпусу. Path – путь к текстам корпуса, vocTrie –17
адрес объекта, в который происходит десериализация дерева дескрипторов, tf – массив
векторов дескрипторов текстов корпуса, fnames – имена файлов корпуса, df, tf_max –
статистика корпуса.
Функция examination(susp_vec, fnames, tf, df, tf_max, result) осуществляет поиск плагиата в
корпусе для текста с вектором дескрипторов susp_vec и выводит результат в файл,
открытый в result.
Функция total_translates(i, vec, fnames, tf, df, tf_max, result) реализует поиск пар дубликатов
внутри корпуса. I – порядковый номер обрабатываемого текста из корпуса.
Рисунок 2 – Архитектура системы
4.4 Характеристики функционирования
4.4.1 Эффективность
При тестировании на корпусе, содержащем 100 пар текстов (ок. 2000 знаков каждый) на
русском и английском языках, разработанная система показала результаты, описанные в
таблице 4
Таблица 4 – Эффективность реализованных алгоритмов
Алгоритм Полнота Точность F-мера
Cosine 0.84 0.97 0.90
iMatch 0.75 0.98 0.85
FreqWord 0.62 1.0 0.76
OptWeight 0.75 0.82 0.78
Как видно из результатов, даже простые методы, такие как косинус или слова с
максимальной частотой, дают хороший результат. Кроме того, методы, разработанные для
моноязыковых корпусов, могут успешно применяться и в случае нескольких языков, при
более мягких критериях похожести текстов.18
Прототип системы разработан без учета каких-либо лингвистических особенностей,
поэтому может быть использован и для других языков.
4.4.2 Производительность
Для измерения производительности был взят корпус, содержащий 100 текстов объемом
около 2000 знаков. По результатам 5 тестов среднее время обработки корпуса составляет 5
часов. Время поиска пар дубликатов для одного текста – около 90 секунд.
Конфигурация тестового компьютера: Intel Core 2 Duo 2GHz, 2048 Mb RAM
Скорость соединения: прием – 0.6 Мб/с, отдача – 10 Мб/с19
Заключение
В рамках данной курсовой работы были получены следующие результаты:
1. Исследованы существующие методы поиска плагиата в многоязычных корпусах
текстов.
2. Разработаны алгоритмы поиска плагиата в корпусах, содержащих тексты на
английском и русском языках.
3. Создан прототип системы поиска плагиата, подтверждающий работоспособность
данного метода.20
Список литературы
1. Зеленков Ю.Г., Сегалович И.В. Cравнительный анализ методов определения
нечетких дубликатов для WEB-документов // Труды 9-ой Всероссийской научной
конференции «Электронные библиотеки: перспективные методы и технологии,
электронные коллекции» RCDL’2007: Сб. работ участников конкурса, том 1.
Переславль-Залесский, Россия: «Университет города Переславля», 2007. С. 166-174
2. Pouliquen Bruno, Ralf Steinberger, Camelia Ignat Automatic Identification of Document
Translations in Large Multilingual Document Collections // Proceedings of the
International Conference Recent Advances in Natural Language Processing
(RANLP'2003). Borovets, Bulgaria: 2003. pp. 401-408
3. Martin Potthast, Alberto Barrón-Cedeño, Benno Stein, Paolo Rosso Cross-language
plagiarism detection // J. Language Resources and Evaluation. 2011. № 45. P. 45-62